{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Publications markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `publications.py`. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one containing your data.\n",
    "\n",
    "TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n",
    "\n",
    "- `excerpt` and `paper_url` can be blank, but the others must have values. \n",
    "- `pub_date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub_date\ttitle\tvenue\texcerpt\tcitation\turl_slug\tpaper_url\n",
      "2020-07-13\tAmortized Population Gibbs Samplers with Neural Sufficient Statistics\t37th International Conference on Machine Learning\tWe develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods. \t\"@InProceedings{pmlr-v119-wu20h,\n",
      "  title = \t {Amortized Population {G}ibbs Samplers with Neural Sufficient Statistics},\n",
      "  author =       {Wu, Hao and Zimmermann, Heiko and Sennesh, Eli and Le, Tuan Anh and Van De Meent, Jan-Willem},\n",
      "  booktitle = \t {Proceedings of the 37th International Conference on Machine Learning},\n",
      "  pages = \t {10421--10431},\n",
      "  year = \t {2020},\n",
      "  editor = \t {III, Hal Daum√© and Singh, Aarti},\n",
      "  volume = \t {119},\n",
      "  series = \t {Proceedings of Machine Learning Research},\n",
      "  month = \t {13--18 Jul},\n",
      "  publisher =    {PMLR},\n",
      "  pdf = \t {http://proceedings.mlr.press/v119/wu20h/wu20h.pdf},\n",
      "  url = \t {https://proceedings.mlr.press/v119/wu20h.html},\n",
      "  abstract = \t {We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.}\n",
      "}\"\tApg-icml-2020\thttp://esennesh.github.io/files/wu20h.pdf\n",
      "2020-11-14\tLearning a Deep Generative Model like a Program: the Free Category Prior\tAAAI Fall 2020 Symposium on Conceptual Abstraction and Analogy in Natural and Artificial Intelligence\t\"Humans surpass the cognitive abilities of most other animals in our ability to \"\"chunk\"\" concepts into words, and then combine the words to combine the concepts. In this process, we make \"\"infinite use of finite means\"\", enabling us to learn new concepts quickly and nest concepts within each-other. While program induction and synthesis remain at the heart of foundational theories of artificial intelligence, only recently has the community moved forward in attempting to use program learning as a benchmark task itself. The cognitive science community has thus often assumed that if the brain has simulation and reasoning capabilities equivalent to a universal computer, then it must employ a serialized, symbolic representation. Here we confront that assumption, and provide a counterexample in which compositionality is expressed via network structure: the free category prior over programs. We show how our formalism allows neural networks to serve as primitives in probabilistic programs. We learn both program structure and model parameters end-to-end. \"\t\"@article{sennesh2020learning,\n",
      "  title={Learning a Deep Generative Model like a Program: the Free Category Prior},\n",
      "  author={Sennesh, Eli},\n",
      "  journal={arXiv preprint arXiv:2011.11063},\n",
      "  year={2020}\n",
      "}\"\tfreecat-aaai-2020-symposium\thttp://esennesh.github.io/files/freecat_aaai_symposium_2020.pdf\n",
      "2020-12-06\tNeural Topographic Factor Analysis for fMRI Data\tAdvances in Neural Information Processing Systems 33 (NeurIPS 2020) \tNeuroimaging studies produce gigabytes of spatio-temporal data for a small number of participants and stimuli. Recent work increasingly suggests that the common practice of averaging across participants and stimuli leaves out systematic and meaningful information. We propose Neural Topographic Factor Analysis (NTFA), a probabilistic factor analysis model that infers embeddings for participants and stimuli. These embeddings allow us to reason about differences between participants and stimuli as signal rather than noise. We evaluate NTFA on data from an in-house pilot experiment, as well as two publicly available datasets. We demonstrate that inferring representations for participants and stimuli improves predictive generalization to unseen data when compared to previous topographic methods. We also demonstrate that the inferred latent factor representations are useful for downstream tasks such as multivoxel pattern analysis and functional connectivity.\t\"@inproceedings{NEURIPS2020_8c3c27ac,\n",
      " author = {Sennesh, Eli and Khan, Zulqarnain and Wang, Yiyu and Hutchinson, J Benjamin and Satpute, Ajay and Dy, Jennifer and van de Meent, Jan-Willem},\n",
      " booktitle = {Advances in Neural Information Processing Systems},\n",
      " editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n",
      " pages = {12046--12056},\n",
      " publisher = {Curran Associates, Inc.},\n",
      " title = {Neural Topographic Factor Analysis for fMRI Data},\n",
      " url = {https://proceedings.neurips.cc/paper/2020/file/8c3c27ac7d298331a1bdfd0a5e8703d3-Paper.pdf},\n",
      " volume = {33},\n",
      " year = {2020}\n",
      "}\"\tNtfa-neurips-2020\thttp://esennesh.github.io/files/ntfa_neurips_2020.pdf\n",
      "2021-07-27\tLearning proposals for probabilistic programs with inference combinators\tProceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence\t We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing. \t\"@InProceedings{pmlr-v161-stites21a,\n",
      "  title = \t {Learning proposals for probabilistic programs with inference combinators},\n",
      "  author =       {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},\n",
      "  booktitle = \t {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},\n",
      "  pages = \t {1056--1066},\n",
      "  year = \t {2021},\n",
      "  editor = \t {de Campos, Cassio and Maathuis, Marloes H.},\n",
      "  volume = \t {161},\n",
      "  series = \t {Proceedings of Machine Learning Research},\n",
      "  month = \t {27--30 Jul},\n",
      "  publisher =    {PMLR},\n",
      "  pdf = \t {https://proceedings.mlr.press/v161/stites21a/stites21a.pdf},\n",
      "  url = \t {https://proceedings.mlr.press/v161/stites21a.html},\n",
      "  abstract = \t {We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.}\n",
      "}\n",
      "\"\tCombinators-uai-2021\thttp://esennesh.github.io/files/stites21a.pdf\n",
      "2021-12-20\tInteroception as modeling, allostasis as control\tBiological Psychology\tThe brain regulates the body by anticipating its needs and attempting to meet them before they arise ‚Äì a process called allostasis. Allostasis requires a model of the changing sensory conditions within the body, a process called interoception. In this paper, we examine how interoception may provide performance feedback for allostasis. We suggest studying allostasis in terms of control theory, reviewing control theory‚Äôs applications to related issues in physiology, motor control, and decision making. We synthesize these by relating them to the important properties of allostatic regulation as a control problem. We then sketch a novel formalism for how the brain might perform allostatic control of the viscera by analogy to skeletomotor control, including a mathematical view on how interoception acts as performance feedback for allostasis. Finally, we suggest ways to test implications of our hypotheses.\t\"@article{sennesh2022interoception,\n",
      "  title={Interoception as modeling, allostasis as control},\n",
      "  author={Sennesh, Eli and Theriault, Jordan and Brooks, Dana and van de Meent, Jan-Willem and Barrett, Lisa Feldman and Quigley, Karen S},\n",
      "  journal={Biological Psychology},\n",
      "  volume={167},\n",
      "  pages={108242},\n",
      "  year={2022},\n",
      "  publisher={Elsevier}\n",
      "}\"\tAllostasis-biopsych-2021\thttp://esennesh.github.io/files/allostasis_biopsych_2021.pdf\n",
      "2022-03-29\tA Computational Neural Model for Mapping Degenerate Neural Architectures\tNeuroinformatics\tDegeneracy in biological systems refers to a many-to-one mapping between physical structures and their functional (including psychological) outcomes. Despite the ubiquity of the phenomenon, traditional analytical tools for modeling degeneracy in neuroscience are extremely limited. In this study, we generated synthetic datasets to describe three situations of degeneracy in fMRI data to demonstrate the limitations of the current univariate approach. We describe a novel computational approach for the analysis referred to as neural topographic factor analysis (NTFA). NTFA is designed to capture variations in neural activity across task conditions and participants. The advantage of this discovery-oriented approach is to reveal whether and how experimental trials and participants cluster into task conditions and participant groups. We applied NTFA on simulated data, revealing the appropriate degeneracy assumption ‚Ä¶\t\"@article{khan2022computational,\n",
      "  title={A computational neural model for mapping degenerate neural architectures},\n",
      "  author={Khan, Zulqarnain and Wang, Yiyu and Sennesh, Eli and Dy, Jennifer and Ostadabbas, Sarah and van de Meent, Jan-Willem and Hutchinson, J Benjamin and Satpute, Ajay B},\n",
      "  journal={Neuroinformatics},\n",
      "  pages={1--15},\n",
      "  year={2022},\n",
      "  publisher={Springer}\n",
      "}\"\tNtfa-neuroinformatics-2022\thttp://esennesh.github.io/files/ntfa_neuroinformatics_2022.pdf\n",
      "2022-05-09\tA Probabilistic Generative Model of Free Categories\tarxiv\tApplied category theory has recently developed libraries for computing with morphisms in interesting categories, while machine learning has developed ways of learning programs in interesting languages. Taking the analogy between categories and languages seriously, this paper defines a probabilistic generative model of morphisms in free monoidal categories over domain-specific generating objects and morphisms. The paper shows how acyclic directed wiring diagrams can model specifications for morphisms, which the model can use to generate morphisms. Amortized variational inference in the generative model then enables learning of parameters (by maximum likelihood) and inference of latent variables (by Bayesian inversion). A concrete experiment shows that the free category prior achieves competitive reconstruction performance on the Omniglot dataset.\t\"@article{sennesh2022probabilistic,\n",
      "  title={A Probabilistic Generative Model of Free Categories},\n",
      "  author={Sennesh, Eli and Xu, Tom and Maruyama, Yoshihiro},\n",
      "  journal={arXiv preprint arXiv:2205.04545},\n",
      "  year={2022}\n",
      "}\"\tFreecat-arxiv-2022\thttp://esennesh.github.io/files/freecat_arxiv_2022.pdf\n"
     ]
    }
   ],
   "source": [
    "!cat publications.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "\n",
    "We are using the very handy pandas library for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": true,
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>citation</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>Amortized Population Gibbs Samplers with Neura...</td>\n",
       "      <td>37th International Conference on Machine Learning</td>\n",
       "      <td>We develop amortized population Gibbs (APG) sa...</td>\n",
       "      <td>@InProceedings{pmlr-v119-wu20h,\\n  title = \\t ...</td>\n",
       "      <td>Apg-icml-2020</td>\n",
       "      <td>http://esennesh.github.io/files/wu20h.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>Learning a Deep Generative Model like a Progra...</td>\n",
       "      <td>AAAI Fall 2020 Symposium on Conceptual Abstrac...</td>\n",
       "      <td>Humans surpass the cognitive abilities of most...</td>\n",
       "      <td>@article{sennesh2020learning,\\n  title={Learni...</td>\n",
       "      <td>freecat-aaai-2020-symposium</td>\n",
       "      <td>http://esennesh.github.io/files/freecat_aaai_s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-06</td>\n",
       "      <td>Neural Topographic Factor Analysis for fMRI Data</td>\n",
       "      <td>Advances in Neural Information Processing Syst...</td>\n",
       "      <td>Neuroimaging studies produce gigabytes of spat...</td>\n",
       "      <td>@inproceedings{NEURIPS2020_8c3c27ac,\\n author ...</td>\n",
       "      <td>Ntfa-neurips-2020</td>\n",
       "      <td>http://esennesh.github.io/files/ntfa_neurips_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>Learning proposals for probabilistic programs ...</td>\n",
       "      <td>Proceedings of the Thirty-Seventh Conference o...</td>\n",
       "      <td>We develop operators for construction of prop...</td>\n",
       "      <td>@InProceedings{pmlr-v161-stites21a,\\n  title =...</td>\n",
       "      <td>Combinators-uai-2021</td>\n",
       "      <td>http://esennesh.github.io/files/stites21a.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>Interoception as modeling, allostasis as control</td>\n",
       "      <td>Biological Psychology</td>\n",
       "      <td>The brain regulates the body by anticipating i...</td>\n",
       "      <td>@article{sennesh2022interoception,\\n  title={I...</td>\n",
       "      <td>Allostasis-biopsych-2021</td>\n",
       "      <td>http://esennesh.github.io/files/allostasis_bio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>A Computational Neural Model for Mapping Degen...</td>\n",
       "      <td>Neuroinformatics</td>\n",
       "      <td>Degeneracy in biological systems refers to a m...</td>\n",
       "      <td>@article{khan2022computational,\\n  title={A co...</td>\n",
       "      <td>Ntfa-neuroinformatics-2022</td>\n",
       "      <td>http://esennesh.github.io/files/ntfa_neuroinfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>A Probabilistic Generative Model of Free Categ...</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>Applied category theory has recently developed...</td>\n",
       "      <td>@article{sennesh2022probabilistic,\\n  title={A...</td>\n",
       "      <td>Freecat-arxiv-2022</td>\n",
       "      <td>http://esennesh.github.io/files/freecat_arxiv_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pub_date                                              title  \\\n",
       "0  2020-07-13  Amortized Population Gibbs Samplers with Neura...   \n",
       "1  2020-11-14  Learning a Deep Generative Model like a Progra...   \n",
       "2  2020-12-06   Neural Topographic Factor Analysis for fMRI Data   \n",
       "3  2021-07-27  Learning proposals for probabilistic programs ...   \n",
       "4  2021-12-20   Interoception as modeling, allostasis as control   \n",
       "5  2022-03-29  A Computational Neural Model for Mapping Degen...   \n",
       "6  2022-05-09  A Probabilistic Generative Model of Free Categ...   \n",
       "\n",
       "                                               venue  \\\n",
       "0  37th International Conference on Machine Learning   \n",
       "1  AAAI Fall 2020 Symposium on Conceptual Abstrac...   \n",
       "2  Advances in Neural Information Processing Syst...   \n",
       "3  Proceedings of the Thirty-Seventh Conference o...   \n",
       "4                              Biological Psychology   \n",
       "5                                   Neuroinformatics   \n",
       "6                                              arxiv   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0  We develop amortized population Gibbs (APG) sa...   \n",
       "1  Humans surpass the cognitive abilities of most...   \n",
       "2  Neuroimaging studies produce gigabytes of spat...   \n",
       "3   We develop operators for construction of prop...   \n",
       "4  The brain regulates the body by anticipating i...   \n",
       "5  Degeneracy in biological systems refers to a m...   \n",
       "6  Applied category theory has recently developed...   \n",
       "\n",
       "                                            citation  \\\n",
       "0  @InProceedings{pmlr-v119-wu20h,\\n  title = \\t ...   \n",
       "1  @article{sennesh2020learning,\\n  title={Learni...   \n",
       "2  @inproceedings{NEURIPS2020_8c3c27ac,\\n author ...   \n",
       "3  @InProceedings{pmlr-v161-stites21a,\\n  title =...   \n",
       "4  @article{sennesh2022interoception,\\n  title={I...   \n",
       "5  @article{khan2022computational,\\n  title={A co...   \n",
       "6  @article{sennesh2022probabilistic,\\n  title={A...   \n",
       "\n",
       "                      url_slug  \\\n",
       "0                Apg-icml-2020   \n",
       "1  freecat-aaai-2020-symposium   \n",
       "2            Ntfa-neurips-2020   \n",
       "3         Combinators-uai-2021   \n",
       "4     Allostasis-biopsych-2021   \n",
       "5   Ntfa-neuroinformatics-2022   \n",
       "6           Freecat-arxiv-2022   \n",
       "\n",
       "                                           paper_url  \n",
       "0          http://esennesh.github.io/files/wu20h.pdf  \n",
       "1  http://esennesh.github.io/files/freecat_aaai_s...  \n",
       "2  http://esennesh.github.io/files/ntfa_neurips_2...  \n",
       "3      http://esennesh.github.io/files/stites21a.pdf  \n",
       "4  http://esennesh.github.io/files/allostasis_bio...  \n",
       "5  http://esennesh.github.io/files/ntfa_neuroinfo...  \n",
       "6  http://esennesh.github.io/files/freecat_arxiv_...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\n",
    "publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for row, item in publications.iterrows():\n",
    "    \n",
    "    md_filename = str(item.pub_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.pub_date) + \"-\" + item.url_slug\n",
    "    year = item.pub_date[:4]\n",
    "    \n",
    "    ## YAML variables\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    \n",
    "    md += \"\"\"collection: publications\"\"\"\n",
    "    \n",
    "    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n",
    "    \n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\nexcerpt: '\" + html_escape(item.excerpt) + \"'\"\n",
    "    \n",
    "    md += \"\\ndate: \" + str(item.pub_date) \n",
    "    \n",
    "    md += \"\\nvenue: '\" + html_escape(item.venue) + \"'\"\n",
    "    \n",
    "    if len(str(item.paper_url)) > 5:\n",
    "        md += \"\\npaperurl: '\" + item.paper_url + \"'\"\n",
    "    \n",
    "    md += \"\\ncitation: '\" + html_escape(item.citation) + \"'\"\n",
    "    \n",
    "    md += \"\\n---\"\n",
    "    \n",
    "    ## Markdown description for individual page\n",
    "        \n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n",
    "    \n",
    "    if len(str(item.paper_url)) > 5:\n",
    "        md += \"\\n[Download paper here](\" + item.paper_url + \")\\n\" \n",
    "        \n",
    "    md += \"\\nRecommended citation: \" + item.citation\n",
    "    \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "       \n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the publications directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-04-08-structured-gotos-slightly-harmful.md\n",
      "2018-05-02-creer-de-tete-de-nombreux-mots-de-passe-inviolables-et-inoubliables.md\n",
      "2018-10-04-combinators-modeling-inference.md\n",
      "2018-10-04-Combinators-probprog-2018.md\n",
      "2018-12-07-bnp-neurips-combinators.md\n",
      "2019-06-24-arxiv-neuraltfa.md\n",
      "2020-07-13-Apg-icml-2020.md\n",
      "2020-11-14-freecat-aaai-2020-symposium.md\n",
      "2020-12-06-Ntfa-neurips-2020.md\n",
      "2021-07-27-Combinators-uai-2021.md\n",
      "2021-12-20-Allostasis-biopsych-2021.md\n",
      "2022-03-29-Ntfa-neuroinformatics-2022.md\n",
      "2022-05-09-Freecat-arxiv-2022.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_publications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Learning proposals for probabilistic programs with inference combinators\"\n",
      "collection: publications\n",
      "permalink: /publication/2021-07-27-Combinators-uai-2021\n",
      "excerpt: ' We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing. '\n",
      "date: 2021-07-27\n",
      "venue: 'Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence'\n",
      "paperurl: 'http://esennesh.github.io/files/stites21a.pdf'\n",
      "citation: '@InProceedings{pmlr-v161-stites21a,\n",
      "  title = \t {Learning proposals for probabilistic programs with inference combinators},\n",
      "  author =       {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},\n",
      "  booktitle = \t {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},\n",
      "  pages = \t {1056--1066},\n",
      "  year = \t {2021},\n",
      "  editor = \t {de Campos, Cassio and Maathuis, Marloes H.},\n",
      "  volume = \t {161},\n",
      "  series = \t {Proceedings of Machine Learning Research},\n",
      "  month = \t {27--30 Jul},\n",
      "  publisher =    {PMLR},\n",
      "  pdf = \t {https://proceedings.mlr.press/v161/stites21a/stites21a.pdf},\n",
      "  url = \t {https://proceedings.mlr.press/v161/stites21a.html},\n",
      "  abstract = \t {We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.}\n",
      "}\n",
      "'\n",
      "---\n",
      " We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing. \n",
      "\n",
      "[Download paper here](http://esennesh.github.io/files/stites21a.pdf)\n",
      "\n",
      "Recommended citation: @InProceedings{pmlr-v161-stites21a,\n",
      "  title = \t {Learning proposals for probabilistic programs with inference combinators},\n",
      "  author =       {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},\n",
      "  booktitle = \t {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},\n",
      "  pages = \t {1056--1066},\n",
      "  year = \t {2021},\n",
      "  editor = \t {de Campos, Cassio and Maathuis, Marloes H.},\n",
      "  volume = \t {161},\n",
      "  series = \t {Proceedings of Machine Learning Research},\n",
      "  month = \t {27--30 Jul},\n",
      "  publisher =    {PMLR},\n",
      "  pdf = \t {https://proceedings.mlr.press/v161/stites21a/stites21a.pdf},\n",
      "  url = \t {https://proceedings.mlr.press/v161/stites21a.html},\n",
      "  abstract = \t {We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ../_publications/2021-07-27-Combinators-uai-2021.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
