---
title: "Learning a Deep Generative Model like a Program: the Free Category Prior"
collection: publications
permalink: /publication/2020-11-14-freecat-aaai-2020-symposium
excerpt: 'Humans surpass the cognitive abilities of most other animals in our ability to &quot;chunk&quot; concepts into words, and then combine the words to combine the concepts. In this process, we make &quot;infinite use of finite means&quot;, enabling us to learn new concepts quickly and nest concepts within each-other. While program induction and synthesis remain at the heart of foundational theories of artificial intelligence, only recently has the community moved forward in attempting to use program learning as a benchmark task itself. The cognitive science community has thus often assumed that if the brain has simulation and reasoning capabilities equivalent to a universal computer, then it must employ a serialized, symbolic representation. Here we confront that assumption, and provide a counterexample in which compositionality is expressed via network structure: the free category prior over programs. We show how our formalism allows neural networks to serve as primitives in probabilistic programs. We learn both program structure and model parameters end-to-end. '
date: 2020-11-14
venue: 'AAAI Fall 2020 Symposium on Conceptual Abstraction and Analogy in Natural and Artificial Intelligence'
paperurl: 'http://esennesh.github.io/files/freecat_aaai_symposium_2020.pdf'
citation: '@article{sennesh2020learning,
  title={Learning a Deep Generative Model like a Program: the Free Category Prior},
  author={Sennesh, Eli},
  journal={arXiv preprint arXiv:2011.11063},
  year={2020}
}'
---
Humans surpass the cognitive abilities of most other animals in our ability to &quot;chunk&quot; concepts into words, and then combine the words to combine the concepts. In this process, we make &quot;infinite use of finite means&quot;, enabling us to learn new concepts quickly and nest concepts within each-other. While program induction and synthesis remain at the heart of foundational theories of artificial intelligence, only recently has the community moved forward in attempting to use program learning as a benchmark task itself. The cognitive science community has thus often assumed that if the brain has simulation and reasoning capabilities equivalent to a universal computer, then it must employ a serialized, symbolic representation. Here we confront that assumption, and provide a counterexample in which compositionality is expressed via network structure: the free category prior over programs. We show how our formalism allows neural networks to serve as primitives in probabilistic programs. We learn both program structure and model parameters end-to-end. 

[Download paper here](http://esennesh.github.io/files/freecat_aaai_symposium_2020.pdf)

Recommended citation: @article{sennesh2020learning,
  title={Learning a Deep Generative Model like a Program: the Free Category Prior},
  author={Sennesh, Eli},
  journal={arXiv preprint arXiv:2011.11063},
  year={2020}
}